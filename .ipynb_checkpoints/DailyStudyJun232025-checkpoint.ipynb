{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351569c0-19db-42e4-ba99-a5abcb71afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_ollama import ChatOllama  # or whatever LLM provider you're using\n",
    "\n",
    "# Initialize your LLM - adjust model name and add your API key as needed\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\", temperature=0.3, timeout=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b88897f-7f75-42c3-9b61-5b51c3f5d0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user wants me to explain machine learning in exactly two sentences. Let me start by recalling the basic definition. Machine learning is a subset of artificial intelligence where systems learn from data without explicit programming. \n",
      "\n",
      "Now, I need to make sure it's concise. The first sentence covers the core concept. The second sentence should highlight the key aspect, maybe the learning process and the use of data. I should avoid technical jargon but keep it accurate. Let me check if \"without explicit programming\" is correct. Yes, that's right. Also, mention that the system improves over time by learning patterns from data. That's a good point. \n",
      "\n",
      "Wait, the user might be looking for a simple yet comprehensive explanation. Let me structure it as: first sentence defines it, second sentence explains how it works. Make sure it's two sentences and exactly two. No markdown, just plain text. Alright, that should do it.\n",
      "</think>\n",
      "\n",
      "Machine learning is a subset of artificial intelligence that enables systems to learn from data, identify patterns, and make decisions with minimal human intervention. It involves training models on large datasets to improve their performance on specific tasks over time.\n"
     ]
    }
   ],
   "source": [
    "# Simple test: create a basic prompt and chain\n",
    "test_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in exactly 2 sentences.\"\n",
    ")\n",
    "\n",
    "# Create a simple chain\n",
    "simple_chain = test_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Test it\n",
    "result = simple_chain.invoke({\"topic\": \"machine learning\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1581076-b7a6-482f-afae-a97689e11a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 of our pipeline: Code Analysis\n",
    "analysis_prompt = PromptTemplate(\n",
    "    input_variables=[\"code\"],\n",
    "    template=\"\"\"Analyze this code and tell me:\n",
    "1. What it does (brief)\n",
    "2. Any obvious inefficiencies\n",
    "\n",
    "Code:{code}. \n",
    "Keep your response short and direct.\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8213d7bf-381c-42bc-85bd-dd7941d496c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANALYSIS RESULT:\n",
      "<think>\n",
      "Okay, let's see. The user provided a Python function called find_max that takes a list of numbers and returns the maximum value. The code starts by setting max_val to the first element of the list. Then it loops through each element using a for loop. If the current number is larger than the current max_val, it updates max_val. Finally, it returns the max_val.\n",
      "\n",
      "First, the question is asking for two things: what the code does and any obvious inefficiencies. Let's break it down.\n",
      "\n",
      "What does the code do? Well, it's a straightforward implementation of finding the maximum in a list. It initializes the max_val with the first element, then iterates through each element, updating max_val if a larger one is found. So it's basically the standard approach for finding the maximum in a list. So the first part is clear.\n",
      "\n",
      "Now, inefficiencies. The main thing here might be the use of a for loop. Wait, the code uses a for loop with range(len(numbers)), which is okay. But in Python, the list's index starts at 0, so the first element is index 0. The code is correct there. But maybe the loop is redundant? Because the first element is already set as max_val, so the loop can start from index 1. Wait, no. The code starts with max_val = numbers[0], then loops from i=0 to len(numbers)-1. Wait, the range is len(numbers), so if the list is empty, it would crash. But the problem probably assumes the list is non-empty. So the code is correct.\n",
      "\n",
      "But maybe the loop is unnecessary. Because once the first element is set as max_val, the rest of the elements are compared. But the code is checking all elements, even the first one. Wait, no. The first element is already the max_val, so the loop will check each element. But if the list has only one element, the loop doesn't run, and returns that element. So that's correct.\n",
      "\n",
      "But in terms of efficiency, the code is O(n) time, which is acceptable. However, in Python, the list's index starts at 0, so the code is correct. But maybe there's a way to optimize. For example, if the list is empty, it would crash. But the problem probably assumes the list is non-empty. So the code is correct.\n",
      "\n",
      "Wait, but the code is using a for loop that starts at 0. So it's checking all elements. But maybe the code could be optimized by starting the loop from index 1, but that would require checking if the list has at least one element. But the code is correct as is.\n",
      "\n",
      "Another possible inefficiency: the code uses a for loop, but in Python, the list's index is 0-based. So the code is correct. But maybe the code could be written with a while loop, but that's not necessary here.\n",
      "\n",
      "Alternatively, the code could be written with a built-in function like max(), but that's not the case here. The user is asking for inefficiencies in the given code, not suggesting alternatives.\n",
      "\n",
      "So the answer would be that the code finds the maximum in a list by iterating through each element, starting with the first one. It's O(n) time, which is efficient. But maybe the loop is redundant because the first element is already the max, but in the code, the loop runs through all elements. Wait, no. The first element is set as max_val, then the loop starts from i=0. So the loop checks each element, including the first one. But if the list has only one element, the loop doesn't run, and returns that element. So the code is correct.\n",
      "\n",
      "So the inefficiency might be that the code is checking all elements, but in practice, it's necessary. However, in terms of Python's efficiency, there's no obvious inefficiency here. The code is straightforward and efficient for the given task.\n",
      "</think>\n",
      "\n",
      "1. The code finds the maximum value in a list by iterating through each element and comparing it to the current maximum.  \n",
      "2. Obvious inefficiency: The loop runs through all elements, but the first element is already set as the initial maximum. However, this is necessary for correctness and the code is efficient for typical use cases. No major inefficiency exists in the implementation.\n"
     ]
    }
   ],
   "source": [
    "# Create the analysis chain\n",
    "analysis_chain = analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Test with a simple inefficient example\n",
    "test_code = \"\"\"\n",
    "def find_max(numbers):\n",
    "    max_val = numbers[0]\n",
    "    for i in range(len(numbers)):\n",
    "        if numbers[i] > max_val:\n",
    "            max_val = numbers[i]\n",
    "    return max_val\n",
    "\"\"\"\n",
    "\n",
    "result = analysis_chain.invoke({\"code\": test_code})\n",
    "print(\"ANALYSIS RESULT:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d5d0f4-8d77-4bdc-bf73-505c605a1c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZED CODE:\n",
      "To improve efficiency, we replace the manual loop with Python's built-in `max()` function, which is optimized for performance and handles the maximum calculation in a single, efficient operation.\n",
      "\n",
      "**Improved Code:**\n",
      "```python\n",
      "def find_max(numbers):\n",
      "    return max(numbers)\n",
      "```\n",
      "\n",
      "**Comment:**  \n",
      "The built-in `max()` function is more efficient than the manual loop, as it is implemented in C and optimized for performance. This reduces the code's complexity and ensures better execution speed.\n"
     ]
    }
   ],
   "source": [
    "# Helper function to clean the output (remove <think> tags)\n",
    "def clean_output(text):\n",
    "    # Remove <think>...</think> blocks\n",
    "    import re\n",
    "    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    return cleaned.strip()\n",
    "\n",
    "# Step 2: Create an optimization prompt that uses the analysis\n",
    "optimization_prompt = PromptTemplate(\n",
    "    input_variables=[\"code\", \"analysis\"],\n",
    "    template=\"\"\"Based on this analysis, rewrite the code to be more efficient: \n",
    "    Original code: \n",
    "    {code}\n",
    "    Analysis:\n",
    "    {analysis}\n",
    "\n",
    "    Provide only the improved code with a brief comment explaining the change.\"\"\"\n",
    ")\n",
    "\n",
    "# Create a two-step chain: analyze ‚Üí optimize\n",
    "two_step_chain = (\n",
    "    {\n",
    "        \"code\": RunnablePassthrough(),\n",
    "        \"analysis\": analysis_chain | clean_output\n",
    "    }\n",
    "    | optimization_prompt \n",
    "    | llm \n",
    "    | StrOutputParser() \n",
    "    | clean_output\n",
    ")\n",
    "\n",
    "# Test the full chain\n",
    "result = two_step_chain.invoke(test_code)\n",
    "print(\"OPTIMIZED CODE:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10f23c4-2375-4038-8e69-5dad6276bd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running chain\n",
      "To improve the code's testability, we have made several key structural and functional changes:\n",
      "\n",
      "---\n",
      "\n",
      "### ‚úÖ Refactored Code\n",
      "\n",
      "```python\n",
      "def calculate_total(price, tax_rate=0.08):\n",
      "    \"\"\"\n",
      "    Calculate the total price including tax.\n",
      "\n",
      "    Args:\n",
      "        price (float): The base price before tax.\n",
      "        tax_rate (float, optional): The tax rate to apply. Defaults to 0.08.\n",
      "\n",
      "    Returns:\n",
      "        float: The total price after applying tax.\n",
      "    \"\"\"\n",
      "    return price * tax_rate + price\n",
      "\n",
      "def print_total(total):\n",
      "    \"\"\"\n",
      "    Print the total price with tax formatted as a string.\n",
      "\n",
      "    Args:\n",
      "        total (float): The calculated total price.\n",
      "\n",
      "    Returns:\n",
      "        None: The function performs side effects (printing).\n",
      "    \"\"\"\n",
      "    print(f\"Total with tax: ${total}\")\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### üß© Key Changes and Explanations\n",
      "\n",
      "1. **Breaking Large Functions into Smaller, Focused Functions**:\n",
      "   - The original `calculate_total` function was a single, large function with print statements.\n",
      "   - We split it into two smaller functions:\n",
      "     - `calculate_total()` for business logic (tax calculation and total price).\n",
      "     - `print_total()` for I/O (printing the formatted result).\n",
      "\n",
      "2. **Separating I/O from Business Logic**:\n",
      "   - The `print_total()` function handles the output, while `calculate_total()` handles the computation.\n",
      "   - This makes the code more modular and easier to test.\n",
      "\n",
      "3. **Making Functions Pure**:\n",
      "   - `calculate_total()` is pure: it only returns a value and does not modify any state.\n",
      "   - `print_total()` is not pure: it performs side effects (printing), but it‚Äôs still well-defined and testable.\n",
      "\n",
      "4. **Removing Hard-Coded Values**:\n",
      "   - The original print statement had a hard-coded format string (`\"Total with tax: ${total}\"`).\n",
      "   - We preserved the format string in `print_total()` to maintain consistency and avoid unnecessary changes.\n",
      "\n",
      "5. **Improved Readability and Maintainability**:\n",
      "   - The code is now more readable, with clear separation of concerns.\n",
      "   - It‚Äôs easier to test, as the business logic is isolated from the I/O.\n",
      "\n",
      "---\n",
      "\n",
      "### üìå Summary\n",
      "\n",
      "This refactoring improves the code's structure, testability, and maintainability while preserving the original functionality. It ensures that the business logic is clean and focused, and the I/O is handled in a separate, well-defined function.|finished running\n",
      "REFACTORED FOR TESTABILITY:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREFACTORED FOR TESTABILITY:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n\u001b[0;32m---> 42\u001b[0m cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<think>.*?</think>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text, flags\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mDOTALL)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned)\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Refactor for testability\n",
    "testability_prompt = PromptTemplate(\n",
    "    input_variables=[\"code\"],\n",
    "    template=\"\"\"Refactor this code to make it more unit-testable by:\n",
    "- Breaking large functions into smaller, focused functions\n",
    "- Separating I/O from business logic\n",
    "- Making functions pure (same input = same output) where possible\n",
    "- Removing hard-coded values\n",
    "\n",
    "Original code:\n",
    "{code}\n",
    "\n",
    "Provide the refactored code with brief comments explaining the changes.\"\"\"\n",
    ")\n",
    "\n",
    "# Test with a more complex example that needs refactoring\n",
    "complex_test_code = \"\"\"\n",
    "def calculate_total(price, tax_rate=0.08):\n",
    "    tax = price * tax_rate\n",
    "    total = price + tax\n",
    "    print(f\"Total with tax: ${total}\")\n",
    "    return total\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "refchainparser = StrOutputParser()\n",
    "chunks = []\n",
    "# Test the refactoring step\n",
    "refactor_chain = testability_prompt | llm | refchainparser | clean_output\n",
    "print(\"running chain\")\n",
    "try: \n",
    "    async for chunk in refactor_chain.astream({\"code\":complex_test_code}):\n",
    "        print(chunk, end=\"|\", flush=True)    \n",
    "        chunks.append(chunk)\n",
    "    print(\"finished running\")\n",
    "except Exception as e:\n",
    "    print(f\"chain failed. {e}\")\n",
    "\n",
    "print(\"REFACTORED FOR TESTABILITY:\")\n",
    "text = ''.join(chunks)\n",
    "cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9843f5f-8a6c-45aa-8cc5-c5e951dae8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating unit tests...\n",
      "GENERATED TESTS:\n",
      "```python\n",
      "import pytest\n",
      "\n",
      "def calculate_tax(price, tax_rate):\n",
      "    return price * tax_rate\n",
      "\n",
      "def calculate_total(price, tax_rate):\n",
      "    return price + calculate_tax(price, tax_rate)\n",
      "\n",
      "# Test Cases\n",
      "\n",
      "def test_calculate_tax_normal():\n",
      "    \"\"\"Test calculate_tax with valid positive price and tax rate.\"\"\"\n",
      "    assert calculate_tax(100, 0.05) == 5.0\n",
      "\n",
      "def test_calculate_tax_edge_case():\n",
      "    \"\"\"Test calculate_tax with price 0 and tax rate 0.0.\"\"\"\n",
      "    assert calculate_tax(0, 0.0) == 0.0\n",
      "\n",
      "def test_calculate_tax_invalid_input():\n",
      "    \"\"\"Test calculate_tax with non-numeric price.\"\"\"\n",
      "    with pytest.raises(TypeError):\n",
      "        calculate_tax('100', 0.05)\n",
      "\n",
      "def test_calculate_total_normal():\n",
      "    \"\"\"Test calculate_total with valid positive price and tax rate.\"\"\"\n",
      "    assert calculate_total(100, 0.05) == 105.0\n",
      "\n",
      "def test_calculate_total_edge_case():\n",
      "    \"\"\"Test calculate_total with price 0 and tax rate 0.0.\"\"\"\n",
      "    assert calculate_total(0, 0.0) == 0.0\n",
      "\n",
      "def test_calculate_total_invalid_input():\n",
      "    \"\"\"Test calculate_total with non-numeric price.\"\"\"\n",
      "    with pytest.raises(TypeError):\n",
      "        calculate_total('100', 0.05)\n",
      "\n",
      "def test_calculate_total_tax_rate_1():\n",
      "    \"\"\"Test calculate_total with tax rate 1.0 (double the price).\"\"\"\n",
      "    assert calculate_total(100, 1.0) == 200.0\n",
      "\n",
      "def test_calculate_tax_negative_price():\n",
      "    \"\"\"Test calculate_tax with negative price.\"\"\"\n",
      "    assert calculate_tax(-100, 0.05) == -5.0\n",
      "\n",
      "def test_calculate_tax_negative_tax_rate():\n",
      "    \"\"\"Test calculate_tax with negative tax rate.\"\"\"\n",
      "    assert calculate_tax(100, -0.05) == -5.0\n",
      "\n",
      "def test_calculate_tax_invalid_tax_rate():\n",
      "    \"\"\"Test calculate_tax with non-numeric tax rate.\"\"\"\n",
      "    with pytest.raises(TypeError):\n",
      "        calculate_tax(100, '0.05')\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### ‚úÖ Summary of Test Coverage\n",
      "\n",
      "- **Normal Cases**:\n",
      "  - `calculate_tax(100, 0.05)` ‚Üí `5.0`\n",
      "  - `calculate_total(100, 0.05)` ‚Üí `105.0`\n",
      "  - `calculate_tax(0, 0.0)` ‚Üí `0.0`\n",
      "  - `calculate_total(0, 0.0)` ‚Üí `0.0`\n",
      "\n",
      "- **Edge Cases**:\n",
      "  - `calculate_tax` with `price = 0` or `tax_rate = 0`\n",
      "  - `calculate_total` with `price = 0` or `tax_rate = 0`\n",
      "  - `calculate_tax` with `price = -100` or `tax_rate = -0.05`\n",
      "\n",
      "- **Invalid Inputs**:\n",
      "  - `calculate_tax` with non-numeric `price` (e.g., `'100'`)\n",
      "  - `calculate_total` with non-numeric `price` (e.g., `'100'`)\n",
      "  - `calculate_tax` with non-numeric `tax_rate` (e.g., `'0.05'`)\n",
      "  - `calculate_total` with non-numeric `tax_rate` (e.g., `'0.05'`)\n",
      "\n",
      "- **Special Cases**:\n",
      "  - `calculate_total` with `tax_rate = 1.0` (doubles the price)\n",
      "  - `calculate_tax` with negative `price` or `tax_rate`\n",
      "\n",
      "---\n",
      "\n",
      "### üß™ Test Names and Purpose\n",
      "\n",
      "- `test_calculate_tax_normal()` ‚Äì Test with valid inputs.\n",
      "- `test_calculate_tax_edge_case()` ‚Äì Test with edge values (0, 0.0).\n",
      "- `test_calculate_tax_invalid_input()` ‚Äì Test with invalid types.\n",
      "- `test_calculate_total_normal()` ‚Äì Test with valid inputs.\n",
      "- `test_calculate_total_edge_case()` ‚Äì Test with edge values (0, 0.0).\n",
      "- `test_calculate_total_invalid_input()` ‚Äì Test with invalid types.\n",
      "- `test_calculate_total_tax_rate_1()` ‚Äì Test with `tax_rate = 1.0`.\n",
      "- `test_calculate_tax_negative_price()` ‚Äì Test with negative price.\n",
      "- `test_calculate_tax_negative_tax_rate()` ‚Äì Test with negative tax rate.\n",
      "- `test_calculate_tax_invalid_tax_rate()` ‚Äì Test with non-numeric tax rate.\n",
      "\n",
      "This comprehensive set of tests ensures robustness and correctness for all possible scenarios.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Generate unit tests for the refactored code\n",
    "test_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"code\"],\n",
    "    template=\"\"\"Generate comprehensive unit tests for this code using pytest.\n",
    "Include:\n",
    "- Test normal cases\n",
    "- Test edge cases\n",
    "- Test invalid inputs\n",
    "- Clear test names\n",
    "\n",
    "Code to test:\n",
    "{code}\n",
    "\n",
    "Provide complete test code that can be saved to a separate file.\"\"\"\n",
    ")\n",
    "\n",
    "# Extract just the refactored code from the previous result\n",
    "# (We'll need to parse this from your result above)\n",
    "refactored_functions = \"\"\"\n",
    "def calculate_tax(price, tax_rate):\n",
    "    return price * tax_rate\n",
    "\n",
    "def calculate_total(price, tax_rate):\n",
    "    return price + calculate_tax(price, tax_rate)\n",
    "\"\"\"\n",
    "\n",
    "# Generate tests\n",
    "test_generation_chain = test_generation_prompt | llm | StrOutputParser() | clean_output\n",
    "\n",
    "print(\"Generating unit tests...\")\n",
    "test_code = test_generation_chain.invoke({\"code\": refactored_functions})\n",
    "print(\"GENERATED TESTS:\")\n",
    "print(test_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41e8451-6cb2-4b35-a224-fbd8da332ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üß™ READY TO SAVE AND RUN TESTS\n",
      "==================================================\n",
      "I can save these tests to 'test_refactored_code.py' and run them.\n",
      "This will:\n",
      "1. Save the test code to a .py file\n",
      "2. Run pytest on the file\n",
      "3. Show you the test results\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you want me to save and run these tests? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Permission granted! Proceeding with file creation and test execution...\n",
      "üìÅ Test file saved as 'test/test_refactored_code.py'\n",
      "üöÄ Running tests...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Ask user permission to save and run tests\n",
    "def extract_code_from_markdown(text):\n",
    "    \"\"\"Extract Python code from markdown code blocks.\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Look for ```python...``` or just ```...``` blocks\n",
    "    python_pattern = r'```python\\s*\\n(.*?)\\n```'\n",
    "    general_pattern = r'```\\s*\\n(.*?)\\n```'\n",
    "    \n",
    "    # Try Python-specific first\n",
    "    python_match = re.search(python_pattern, text, re.DOTALL)\n",
    "    if python_match:\n",
    "        return python_match.group(1).strip()\n",
    "    \n",
    "    # Fall back to general code block\n",
    "    general_match = re.search(general_pattern, text, re.DOTALL)\n",
    "    if general_match:\n",
    "        return general_match.group(1).strip()\n",
    "    \n",
    "    # If no code blocks found, return original (maybe it's already clean)\n",
    "    return text.strip()\n",
    "\n",
    "# clean our freshly produced code\n",
    "test_code = extract_code_from_markdown(test_code)\n",
    "\n",
    "filename = \"test_refactored_code.py\"\n",
    "\n",
    "def ask_permission_and_run_tests(test_code, filename=\"test_refactored_code.py\"):\n",
    "    \"\"\"Ask user permission to save and run the generated tests.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"üß™ READY TO SAVE AND RUN TESTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"I can save these tests to '{filename}' and run them.\")\n",
    "    print(\"This will:\")\n",
    "    print(\"1. Save the test code to a .py file\")\n",
    "    print(\"2. Run pytest on the file\")\n",
    "    print(\"3. Show you the test results\")\n",
    "    \n",
    "    # In a real interactive environment, you'd get user input\n",
    "    # For now, let's assume permission granted\n",
    "    permission = input(\"\\nDo you want me to save and run these tests? (y/n): \").lower().strip()\n",
    "    \n",
    "    if permission == 'y' or permission == 'yes':\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Tests not saved or run.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Test the permission function\n",
    "if ask_permission_and_run_tests(test_code):\n",
    "    print(\"‚úÖ Permission granted! Proceeding with file creation and test execution...\")\n",
    "    with open(filename, \"w\") as f:    \n",
    "        # Save the test file\n",
    "        f.write(test_code)\n",
    "        print(\"üìÅ Test file saved as 'test/test_refactored_code.py'\")\n",
    "        f.close()\n",
    "    \n",
    "    # Run the tests (we'll add this next)\n",
    "    print(\"üöÄ Running tests...\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping test execution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7449f58e-0f11-40aa-a074-4fe77756c7ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3196246639.py, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 36\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(\"‚ùå Some tests failed or there were errors.\")\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_tests_and_show_results(test_filename=\"test_refactored_code.py\"):\n",
    "    \"\"\"Run the pytest tests and display results.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(test_filename):\n",
    "        print(f\"‚ùå Test file '{test_filename}' not found!\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Run pytest with verbose output\n",
    "        result = await subprocess.run(\n",
    "            [\"python\", \"-m\", \"pytest\", test_filename, \"-v\"], \n",
    "            capture_output=True, \n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üß™ TEST RESULTS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        print(\"STDOUT:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        if result.stderr:\n",
    "            print(\"STDERR:\")\n",
    "            print(result.stderr)\n",
    "        \n",
    "        print(f\"\\nüìä Return code: {result.returncode}\")\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ All tests passed!\")\n",
    "        else:\n",
    "            print(\"‚ùå Some tests failed or there were errors.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3580955c-cb06-4370-86dc-cd1d2da92f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
