{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bba997f-51a6-4b2a-b208-c0a2f3d38979",
   "metadata": {
    "id": "0bba997f-51a6-4b2a-b208-c0a2f3d38979"
   },
   "source": [
    "# Simple Agent Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08565675-664e-4cd0-b460-2f3cb72be7a0",
   "metadata": {
    "id": "08565675-664e-4cd0-b460-2f3cb72be7a0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d7553c-57a8-4586-bafb-a27e0037f05d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61d7553c-57a8-4586-bafb-a27e0037f05d",
    "outputId": "15ceff05-71c1-42de-b012-1c9979228228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langgraph langchain\n",
    "%pip install -qU langchain-ollama langchain-community duckduckgo-search\n",
    "%pip install -qU langchain-text-splitters\n",
    "from langchain_core.messages.utils import (\n",
    "    trim_messages,\n",
    "    count_tokens_approximately\n",
    ")\n",
    "import asyncio, random\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "# 'short-term memory\n",
    "# https://api.python.langchain.com/en/latest/checkpoint/langchain_postgres.checkpoint.PostgresSaver.html for postgres\n",
    "#\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    "    trim_messages,\n",
    ")\n",
    "from langchain_core.messages.utils import count_tokens_approximately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62675195-8030-47a0-81b4-a023e71200ce",
   "metadata": {
    "id": "62675195-8030-47a0-81b4-a023e71200ce"
   },
   "outputs": [],
   "source": [
    "memory = MemorySaver()\n",
    "search = DuckDuckGoSearchRun()\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "agent_prompt = \"\"\"\n",
    "    You are a helpful intelligent agent.\n",
    "\"\"\"\n",
    "\n",
    "max_iterations = 3\n",
    "recursion_limit = 2 * max_iterations + 1\n",
    "\n",
    "##########<tools>###\n",
    "\n",
    "\n",
    "\n",
    "##########</tools>###\n",
    "\n",
    "# This function will be called every time before the node that calls LLM\n",
    "# right now, it trims messages when there is not enough memory\n",
    "# the context length for this model is ~40k, so I went with 20k. Adjust as desired.\n",
    "def pre_model_hook(state):\n",
    "    trimmed_messages = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=20000,\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    # You can return updated messages either under `llm_input_messages` or\n",
    "    # `messages` key (see the note below)\n",
    "    return {\"llm_input_messages\": trimmed_messages}\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[search],\n",
    "    prompt=agent_prompt,\n",
    "    pre_model_hook=pre_model_hook,\n",
    "    checkpointer=memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b628fe-9e6e-4faf-9115-30c262289b42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0b628fe-9e6e-4faf-9115-30c262289b42",
    "outputId": "63ea7c3c-e98f-4674-b67e-0fcda29bebc4"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User name:  ketily\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ketily. How can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  I'm curious what plants are in season in Portland, OR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "<sender>ketily</sender><message>I'm curious what plants are in season in Portland, OR</message>\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking about plants in season in Portland, OR. I need to figure out what tools to use here. The available tool is duckduckgo_search, which is for current events. So I should perform a search to find out what's happening in Portland's plant seasons.\n",
      "\n",
      "First, I'll construct the search query. The user mentioned Portland, OR, so the query should include those locations. Maybe something like \"plants in season Portland OR 2023\" to get the latest information. I'll use that as the query parameter.\n",
      "\n",
      "I need to make sure the search is specific enough to get relevant results. Since the user is interested in seasonal plants, the search should focus on current seasons. Also, considering the time frame, using 2023 might be necessary if the current year is 2024. Wait, but the user didn't specify the year, so maybe just \"plants in season Portland OR\" would be better. However, including the year might improve accuracy. But since the tool's parameters require a query, I'll go with \"plants in season Portland OR 2023\" to target the current year.\n",
      "\n",
      "Once the search is done, I'll parse the results to extract the relevant information. The answer should list the plants currently in season in Portland. I'll need to check the search results for any mentions of seasonal plants, maybe from local gardens, parks, or agricultural sources. Then, compile that information into a clear response for the user.\n",
      "</think>\n",
      "Tool Calls:\n",
      "  duckduckgo_search (105f00e8-68b8-4f30-8220-e1440433b301)\n",
      " Call ID: 105f00e8-68b8-4f30-8220-e1440433b301\n",
      "  Args:\n",
      "    query: plants in season Portland OR 2023\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: duckduckgo_search\n",
      "\n",
      "Error: DuckDuckGoSearchException(RatelimitException('https://lite.duckduckgo.com/lite/ 202 Ratelimit'))\n",
      " Please fix your mistakes.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user asked about plants in season in Portland, OR. I tried using the duckduckgo_search tool, but got a ratelimit error. Hmm, maybe the API is restricted. Let me think of alternative ways to get the info.\n",
      "\n",
      "Since the search tool is rate-limited, perhaps I can provide a general answer based on common knowledge. I know that Portland has seasonal plants like tulips in spring, cherry blossoms in early spring, and autumn has things like magnolias and ferns. Also, the Pacific Northwest typically has early spring blooms. Maybe list the main seasonal plants and their times.\n",
      "\n",
      "I should structure the response to include the main seasons and typical plants, even if the search didn't fetch real-time data. The user might appreciate a concise overview based on general knowledge. Let me check if there's any other info I can include, like local gardens or specific parks. But since the tool failed, I'll stick to the general knowledge answer.\n",
      "</think>\n",
      "\n",
      " Portland, OR experiences seasonal plant blooms throughout the year, with the following notable seasonal plants:\n",
      "\n",
      "### **Spring (Marchâ€“May)**  \n",
      "- **Tulips & Daffodils**: Early spring blooms in parks like the Portland Parks & Open Space system.  \n",
      "- **Cherry Blossoms**: Japanese cherry trees (e.g., the Portland Japanese Garden) bloom in early spring.  \n",
      "- **Magnolia & Rhododendron**: In late spring, magnolias and rhododendrons add color to landscapes.  \n",
      "\n",
      "### **Summer (Juneâ€“August)**  \n",
      "- **Peaches & Apricots**: Local orchards produce fruit in summer.  \n",
      "- **Lavender & Rosemary**: Gardeners cultivate these herbs in warmer months.  \n",
      "\n",
      "### **Autumn (Septemberâ€“November)**  \n",
      "- **Magnolia & Ferns**: Fall foliage includes magnolias and ferns in parks like the Portland Botanical Gardens.  \n",
      "- **Maple Trees**: Sugar maples and other deciduous trees turn colorful in fall.  \n",
      "\n",
      "### **Winter (Decemberâ€“February)**  \n",
      "- **Evergreens**: Trees like pine and fir remain vibrant despite colder temperatures.  \n",
      "- **Winter Flowers**: Some gardens focus on low-maintenance blooms like winter jasmine or everbearing hydrangeas.  \n",
      "\n",
      "For the latest updates, check local gardening blogs or Portland Parks & Open Space resources. Let me know if you'd like specifics! ðŸŒ¸\n"
     ]
    }
   ],
   "source": [
    "## this holds the entire conversation history fed to the agent for now.\n",
    "messages = []\n",
    "\n",
    "username = input(\"User name: \")\n",
    "greeting = f\"Hello {username}. How can I help you today?\"\n",
    "print(greeting)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        messages.append(HumanMessage(\"<sender>\"+username+\"</sender><message>\"+user_input+\"</message>\"))\n",
    "        result = agent.invoke({\"messages\": messages}, {\"recursionLimit\": recursion_limit, \"thread_id\": username})\n",
    "        for msg in result[\"messages\"]:\n",
    "            msg.pretty_print()\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "    except EOFError:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"Thank you for your service!\"\n",
    "        print(\"User: \" + user_input)\n",
    "\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517290f2-e40e-4093-b0b4-c41b3f9679e1",
   "metadata": {
    "id": "517290f2-e40e-4093-b0b4-c41b3f9679e1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
