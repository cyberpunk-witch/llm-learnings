{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4976488c-db0d-4f95-95ad-1efe621c1ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel in /home/bfhgfe/miniforge3/lib/python3.12/site-packages (0.45.1)\n",
      "Requirement already satisfied: pandas in /home/bfhgfe/miniforge3/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/bfhgfe/miniforge3/lib/python3.12/site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/bfhgfe/miniforge3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/bfhgfe/miniforge3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/bfhgfe/miniforge3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/bfhgfe/miniforge3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metamia to Anki Scraper\n",
    "\n",
    "This script scrapes analogies from metamia.com and exports them to Anki-compatible CSV format.\n",
    "Includes filtering options to help prune problematic entries.\n",
    "\n",
    "Requirements:\n",
    "    pip install requests beautifulsoup4 pandas\n",
    "\n",
    "Usage:\n",
    "    python metamia_scraper.py\n",
    "\"\"\"\n",
    "!pip install wheel\n",
    "!pip install pandas\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import csv\n",
    "from typing import List, Dict, Optional\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MetamiaScraper:\n",
    "    def __init__(self, base_url=\"http://www.metamia.com\", delay=1.0):\n",
    "        self.base_url = base_url\n",
    "        self.delay = delay  # Delay between requests to be respectful\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        \n",
    "    def get_page(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"Fetch and parse a page with error handling and rate limiting.\"\"\"\n",
    "        try:\n",
    "            time.sleep(self.delay)\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_analogy_links(self, soup: BeautifulSoup) -> List[str]:\n",
    "        \"\"\"Extract analogy page links from a listing page.\"\"\"\n",
    "        links = []\n",
    "        # Look for links that match the pattern /critique-*\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/critique-' in href:\n",
    "                full_url = urljoin(self.base_url, href)\n",
    "                links.append(full_url)\n",
    "        return list(set(links))  # Remove duplicates\n",
    "    \n",
    "    def parse_analogy_page(self, url: str) -> Optional[Dict]:\n",
    "        \"\"\"Parse an individual analogy page and extract structured data.\"\"\"\n",
    "        soup = self.get_page(url)\n",
    "        if not soup:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Extract from URL pattern: /critique-CONCEPT-like-ANALOGY-ID\n",
    "            url_parts = url.split('/')[-1]  # Get the last part\n",
    "            if 'critique-' in url_parts:\n",
    "                # Remove 'critique-' prefix and split\n",
    "                content = url_parts.replace('critique-', '')\n",
    "                \n",
    "                # Find 'like' separator\n",
    "                if '-like-' in content:\n",
    "                    parts = content.split('-like-', 1)\n",
    "                    concept = parts[0].replace('-', ' ').strip()\n",
    "                    analogy_with_id = parts[1]\n",
    "                    \n",
    "                    # Remove ID (last part after final dash followed by numbers)\n",
    "                    analogy = re.sub(r'-\\d+$', '', analogy_with_id).replace('-', ' ').strip()\n",
    "                else:\n",
    "                    # Fallback parsing\n",
    "                    concept = content.replace('-', ' ').strip()\n",
    "                    analogy = \"Unknown\"\n",
    "            \n",
    "            # Try to extract additional details from page content\n",
    "            writer = \"Not Stated\"\n",
    "            explanation = \"\"\n",
    "            date = \"\"\n",
    "            \n",
    "            # Look for writer information\n",
    "            writer_elem = soup.find(text=re.compile(r'Writer:'))\n",
    "            if writer_elem:\n",
    "                writer_text = writer_elem.strip()\n",
    "                if '--' in writer_text:\n",
    "                    parts = writer_text.split('--')\n",
    "                    if len(parts) >= 2:\n",
    "                        writer = parts[0].replace('Writer:', '').strip()\n",
    "                        date = parts[1].replace('Date:', '').strip()\n",
    "            \n",
    "            # Look for explanation/critique text\n",
    "            # This might be in various places depending on page structure\n",
    "            content_divs = soup.find_all(['div', 'p'], text=True)\n",
    "            for div in content_divs:\n",
    "                text = div.get_text().strip()\n",
    "                if len(text) > 50 and not any(x in text.lower() for x in ['writer:', 'date:', 'most active']):\n",
    "                    explanation = text[:500]  # Limit length\n",
    "                    break\n",
    "            \n",
    "            return {\n",
    "                'concept': concept.title(),\n",
    "                'analogy': analogy.title(),\n",
    "                'writer': writer,\n",
    "                'date': date,\n",
    "                'explanation': explanation,\n",
    "                'url': url,\n",
    "                'quality_score': self.assess_quality(concept, analogy, explanation)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing {url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def assess_quality(self, concept: str, analogy: str, explanation: str) -> int:\n",
    "        \"\"\"Simple quality assessment to help with filtering.\"\"\"\n",
    "        score = 5  # Base score\n",
    "        \n",
    "        # Boost for longer explanations\n",
    "        if len(explanation) > 100:\n",
    "            score += 2\n",
    "        elif len(explanation) > 50:\n",
    "            score += 1\n",
    "            \n",
    "        # Penalize very short or generic analogies\n",
    "        if len(analogy.split()) < 2:\n",
    "            score -= 1\n",
    "        \n",
    "        # Penalize certain problematic patterns (add your own filters here)\n",
    "        problematic_terms = ['fuck', 'shit', 'damn', 'hell']  # Extend as needed\n",
    "        if any(term in analogy.lower() or term in concept.lower() for term in problematic_terms):\n",
    "            score -= 3\n",
    "            \n",
    "        # Boost for scientific/educational content\n",
    "        educational_terms = ['cell', 'dna', 'protein', 'neuron', 'photosynthesis', 'system']\n",
    "        if any(term in concept.lower() for term in educational_terms):\n",
    "            score += 1\n",
    "            \n",
    "        return max(0, min(10, score))  # Clamp to 0-10 range\n",
    "    \n",
    "    def discover_all_analogies(self, max_pages: int = 10) -> List[str]:\n",
    "        \"\"\"Discover analogy URLs by exploring the site structure.\"\"\"\n",
    "        all_links = []\n",
    "        \n",
    "        # Start with the main page\n",
    "        main_soup = self.get_page(self.base_url)\n",
    "        if main_soup:\n",
    "            all_links.extend(self.extract_analogy_links(main_soup))\n",
    "        \n",
    "        # You could add more discovery methods here:\n",
    "        # - Browse by category pages\n",
    "        # - Search results\n",
    "        # - Recent entries pagination\n",
    "        \n",
    "        logger.info(f\"Discovered {len(all_links)} analogy URLs\")\n",
    "        return all_links[:max_pages * 50]  # Reasonable limit\n",
    "    \n",
    "    def scrape_analogies(self, max_entries: int = 200) -> List[Dict]:\n",
    "        \"\"\"Main scraping method.\"\"\"\n",
    "        logger.info(\"Starting Metamia scrape...\")\n",
    "        \n",
    "        # Discover analogy URLs\n",
    "        urls = self.discover_all_analogies()\n",
    "        \n",
    "        analogies = []\n",
    "        for i, url in enumerate(urls[:max_entries]):\n",
    "            logger.info(f\"Scraping {i+1}/{min(max_entries, len(urls))}: {url}\")\n",
    "            \n",
    "            analogy_data = self.parse_analogy_page(url)\n",
    "            if analogy_data:\n",
    "                analogies.append(analogy_data)\n",
    "                \n",
    "        logger.info(f\"Successfully scraped {len(analogies)} analogies\")\n",
    "        return analogies\n",
    "    \n",
    "    def filter_analogies(self, analogies: List[Dict], min_quality: int = 4) -> List[Dict]:\n",
    "        \"\"\"Filter analogies based on quality and other criteria.\"\"\"\n",
    "        filtered = [a for a in analogies if a['quality_score'] >= min_quality]\n",
    "        logger.info(f\"Filtered {len(analogies)} -> {len(filtered)} analogies (min quality: {min_quality})\")\n",
    "        return filtered\n",
    "    \n",
    "    def export_to_anki_csv(self, analogies: List[Dict], filename: str = \"metamia_analogies.csv\"):\n",
    "        \"\"\"Export analogies to Anki-compatible CSV format.\"\"\"\n",
    "        \n",
    "        # Anki CSV format: Front, Back, Extra fields...\n",
    "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['Front', 'Back', 'Explanation', 'Source', 'Quality', 'URL']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            writer.writeheader()\n",
    "            for analogy in analogies:\n",
    "                writer.writerow({\n",
    "                    'Front': analogy['concept'],\n",
    "                    'Back': f\"{analogy['concept']} is like {analogy['analogy']}\",\n",
    "                    'Explanation': analogy['explanation'][:300],  # Truncate for Anki\n",
    "                    'Source': analogy['writer'],\n",
    "                    'Quality': analogy['quality_score'],\n",
    "                    'URL': analogy['url']\n",
    "                })\n",
    "        \n",
    "        logger.info(f\"Exported {len(analogies)} analogies to {filename}\")\n",
    "    \n",
    "    def export_to_pandas(self, analogies: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Export to pandas DataFrame for further analysis.\"\"\"\n",
    "        return pd.DataFrame(analogies)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    scraper = MetamiaScraper(delay=1.5)  # Be respectful with delays\n",
    "    \n",
    "    # Scrape analogies\n",
    "    analogies = scraper.scrape_analogies(max_entries=100)  # Start small\n",
    "    \n",
    "    if not analogies:\n",
    "        logger.error(\"No analogies found!\")\n",
    "        return\n",
    "    \n",
    "    # Filter for quality\n",
    "    filtered_analogies = scraper.filter_analogies(analogies, min_quality=4)\n",
    "    \n",
    "    # Export to CSV for Anki\n",
    "    scraper.export_to_anki_csv(filtered_analogies)\n",
    "    \n",
    "    # Also save as DataFrame for inspection\n",
    "    df = scraper.export_to_pandas(filtered_analogies)\n",
    "    df.to_csv('metamia_raw_data.csv', index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nScraping Summary:\")\n",
    "    print(f\"Total analogies found: {len(analogies)}\")\n",
    "    print(f\"After filtering: {len(filtered_analogies)}\")\n",
    "    print(f\"Average quality score: {df['quality_score'].mean():.1f}\")\n",
    "    print(f\"\\nTop concepts:\")\n",
    "    print(df['concept'].value_counts().head())\n",
    "    \n",
    "    print(f\"\\nFiles created:\")\n",
    "    print(f\"- metamia_analogies.csv (for Anki import)\")\n",
    "    print(f\"- metamia_raw_data.csv (for inspection)\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634d9a8-e73f-41ec-bc15-ac58a4fdfa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5dedd-93e1-4920-8590-464b6b3b6f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
