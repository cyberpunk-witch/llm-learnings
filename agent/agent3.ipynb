{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf4c3989-ce50-47b5-8f83-9e66ab30aa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/bfhgfe/.local/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: tqdm in /home/bfhgfe/.local/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/bfhgfe/.local/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: click in /home/bfhgfe/.local/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/bfhgfe/.local/lib/python3.10/site-packages (from nltk) (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 16:10:27.817885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755990627.834278    2467 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755990627.839141    2467 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755990627.856721    2467 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755990627.856750    2467 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755990627.856752    2467 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755990627.856753    2467 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-23 16:10:27.865259: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt_tab to /home/bfhgfe/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters:\n",
      "\n",
      "-----------------------------------------------------\n",
      "1\n",
      "['A Path to Beneficial Superintelligence by SingularityNet Ambassadors source: https://medium.com/@singularitynetambassadors/a-path-to-beneficial-superintelligence-13ea8dbab2a9  Dr. Ben Goertzel, the founder and CEO of SingularityNET, was welcomed on stage as a leader in artificial general intelligence, robotics, and computational finance research.', 'I’m going to talk about what we’re doing with the artificial superintelligence alliance, which aims to build decentralized AI and decentralized AGI.', 'But what I’m going to talk about now is how we went from AI to artificial general intelligence to artificial superintelligence without screwing up our lives and doing so in a way that benefits humans and other sentient bei...'] : A Path to Beneficial Superintelligence by SingularityNet Ambassadors source: https://medium.com/@singularitynetambassadors/a-path-to-beneficial-superintelligence-13ea8dbab2a9  Dr. Ben Goertzel, the founder and CEO of SingularityNET, was welcomed on stage as a leader in artificial general intelligence, robotics, and computational finance research.\n",
      "['A Path to Beneficial Superintelligence by SingularityNet Ambassadors source: https://medium.com/@singularitynetambassadors/a-path-to-beneficial-superintelligence-13ea8dbab2a9  Dr. Ben Goertzel, the founder and CEO of SingularityNET, was welcomed on stage as a leader in artificial general intelligence, robotics, and computational finance research.', 'I’m going to talk about what we’re doing with the artificial superintelligence alliance, which aims to build decentralized AI and decentralized AGI.', 'But what I’m going to talk about now is how we went from AI to artificial general intelligence to artificial superintelligence without screwing up our lives and doing so in a way that benefits humans and other sentient bei...'] : I’m going to talk about what we’re doing with the artificial superintelligence alliance, which aims to build decentralized AI and decentralized AGI.\n",
      "['A Path to Beneficial Superintelligence by SingularityNet Ambassadors source: https://medium.com/@singularitynetambassadors/a-path-to-beneficial-superintelligence-13ea8dbab2a9  Dr. Ben Goertzel, the founder and CEO of SingularityNET, was welcomed on stage as a leader in artificial general intelligence, robotics, and computational finance research.', 'I’m going to talk about what we’re doing with the artificial superintelligence alliance, which aims to build decentralized AI and decentralized AGI.', 'But what I’m going to talk about now is how we went from AI to artificial general intelligence to artificial superintelligence without screwing up our lives and doing so in a way that benefits humans and other sentient bei...'] : But what I’m going to talk about now is how we went from AI to artificial general intelligence to artificial superintelligence without screwing up our lives and doing so in a way that benefits humans and other sentient bei...\n",
      "0\n",
      "['He began his speech by expressing how excited he was to be present at the superintelligence summit.', 'Before his presentation, he gave the audience a sneak peek of what was in it for them.', 'What I’m going to talk about is equally exciting, but perhaps a little more predictable.'] : He began his speech by expressing how excited he was to be present at the superintelligence summit.\n",
      "['He began his speech by expressing how excited he was to be present at the superintelligence summit.', 'Before his presentation, he gave the audience a sneak peek of what was in it for them.', 'What I’m going to talk about is equally exciting, but perhaps a little more predictable.'] : Before his presentation, he gave the audience a sneak peek of what was in it for them.\n",
      "['He began his speech by expressing how excited he was to be present at the superintelligence summit.', 'Before his presentation, he gave the audience a sneak peek of what was in it for them.', 'What I’m going to talk about is equally exciting, but perhaps a little more predictable.'] : What I’m going to talk about is equally exciting, but perhaps a little more predictable.\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'np'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m (clustered_chunks[x], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m,y)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m task2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;43mTake this list and copy all the items to a single list - output as JSON ONLY.\u001b[39;49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;43mList: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mclustered_chunks\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclustered_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m result \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m     50\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqwen2.5:0.5b\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     51\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: task2}],\n\u001b[1;32m     52\u001b[0m     options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m, })[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     54\u001b[0m cleaned \u001b[38;5;241m=\u001b[39m clean_structured_output(result)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'np'"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "import ollama\n",
    "import asyncio\n",
    "import time \n",
    "import gc\n",
    "import sys\n",
    "sys.path.insert(0, '/home/bfhgfe/agent')\n",
    "import Tools.search\n",
    "from Tools.search import search_for_evidence, search_recent\n",
    "from Tools.ref import QuickMemory\n",
    "from Tools.clean import clean_structured_output, clean_for_prompt\n",
    "apathto = open('./apathto.txt', 'r')\n",
    "current_text = clean_for_prompt(apathto.read())\n",
    "memory = QuickMemory()\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "# task 1: list claims \n",
    "gc.disable()\n",
    "start_time = time.time()\n",
    "# Step 1: Extract all claims\n",
    "sentences = nltk.sent_tokenize(current_text)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5)\n",
    "clusters = clustering.fit_predict(embeddings)\n",
    "\n",
    "# Group sentences by cluster\n",
    "clustered_chunks = {}\n",
    "for i, cluster_id in enumerate(clusters):\n",
    "    if cluster_id not in clustered_chunks:\n",
    "        clustered_chunks[cluster_id] = []\n",
    "    clustered_chunks[cluster_id].append(sentences[i])\n",
    "\n",
    "print(\"clusters:\\n\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "for x in clustered_chunks:\n",
    "    print (x)\n",
    "    for y in clustered_chunks[x]:\n",
    "        print (clustered_chunks[x]\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "task2 = str.format(f\"\"\"\n",
    "Take this list and copy all the items to a single list - output as JSON ONLY.\n",
    "List: {clustered_chunks}\n",
    "\"\"\", clustered_chunks)\n",
    "\n",
    "result = ollama.chat(\n",
    "    model='qwen2.5:0.5b',\n",
    "    messages=[{'role': 'user', 'content': task2}],\n",
    "    options={'format':'json', 'temperature': 0.1, })['message']['content']\n",
    "\n",
    "cleaned = clean_structured_output(result)\n",
    "if isinstance(cleaned, list):\n",
    "    print(\"\\ndistinct claims: \\n\")\n",
    "    print(cleaned)\n",
    "\n",
    "# step 2: Identify implicit claims\n",
    "task3 = str.format(f\"\"\"\n",
    "Task: Output as a single JSON list only. In your own words, what would need to be true for the claims in this list to be true?\n",
    "Explicit claims: {cleaned}\n",
    "\"\"\", cleaned)\n",
    "\n",
    "result = ollama.chat(\n",
    "    model='qwen2.5:0.5b',\n",
    "    messages=[{'role': 'user', 'content': task2}],\n",
    "    options={'format':'json', 'temperature': 0.1, \"DRY\": 0.3},\n",
    "    stream=False,\n",
    ")['message']['content']\n",
    "\n",
    "implicit = clean_structured_output(result)\n",
    "if isinstance(cleaned, list):\n",
    "    print(\"\\nImplicit claims: \\n\")\n",
    "    print(implicit)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "gc.enable()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# task 2: verify claims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b9f04-44b0-4215-bf14-2ce068823892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba7fa8-db9d-486a-b103-50788b75722d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d1bf5-b094-4de4-87f4-6a54b69d01ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
